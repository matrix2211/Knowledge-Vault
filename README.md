# ğŸ“š Knowledge Vault â€” RAG-Powered Document Q&A

A **Retrieval-Augmented Generation (RAG) application** that allows users to upload documents and ask questions **strictly grounded in their content**. This has been updated was previously a local application running on **LLaMA** but now on **Google Gemini**.

Built with **FastAPI + ChromaDB (Cloud) + Google Gemini**, this project focuses on **correct RAG architecture, file-scoped retrieval, and production-grade guardrails** â€” not just a toy demo.

---

## âœ¨ Features

- ğŸ“‚ Upload and manage multiple PDF documents  
- ğŸ” Ask natural language questions grounded **only in selected files**  
- ğŸ§  Retrieval-Augmented Generation (RAG) using vector similarity search  
- ğŸ—‚ï¸ File-scoped querying (no cross-document leakage)  
- ğŸ§¾ Source attribution per answer  
- ğŸš« Hallucination guardrails for weak or unreadable PDFs  
- ğŸ’¬ ChatGPT-style UI (sidebar + chat + fixed input)  
- â˜ï¸ Cloud-backed vector storage using **ChromaDB Cloud**

---

## ğŸ—ï¸ Architecture Overview

```text
Frontend (HTML / CSS / JS)
â†“
FastAPI Backend
â†“
Google Gemini Embeddings (embedding-001)
â†“
ChromaDB Cloud (Vector Store)
â†“
Google Gemini LLM (gemini-pro)

```

### RAG Flow
1. User uploads a PDF
2. Text is extracted, chunked, embedded
3. Embeddings + metadata stored in ChromaDB Cloud
4. User selects a file + asks a question
5. Retrieval happens **only within that file**
6. Gemini generates an answer strictly from retrieved context

---

## ğŸ“ Project Structure

```text

backend/
â”œâ”€â”€ embeddings.py
â”œâ”€â”€ ingest.py
â”œâ”€â”€ llm.py
â”œâ”€â”€ main.py
â”œâ”€â”€ rag.py
â”œâ”€â”€ vector_store.py
â””â”€â”€ init.py

frontend/
â”œâ”€â”€ index.html
â”œâ”€â”€ style.css
â””â”€â”€ app.js

requirements.txt
README.md

```

## Website UI Preview

![ui](images/ui2.png)
